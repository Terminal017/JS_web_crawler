# JS Web Crawler

ç”± Claude å’Œ Trae ååŠ©å¼€å‘çš„åŸºäº Playwright çš„ TypeScript çˆ¬è™«é¡¹ç›®

## é¡¹ç›®ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ã€é…ç½®çµæ´»çš„ç½‘é¡µçˆ¬è™«å·¥å…·ï¼ŒåŸºäº Playwright å’Œ TypeScript å¼€å‘ã€‚æ”¯æŒé€šè¿‡ JSON é…ç½®æ–‡ä»¶å®šä¹‰çˆ¬å–è§„åˆ™ï¼Œå¯ä»¥è½»æ¾æŠ“å–å„ç§ç½‘ç«™çš„æ•°æ®ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **åŸºäº Playwright**ï¼šæ”¯æŒç°ä»£ JavaScript ç½‘ç«™ï¼Œå¤„ç†åŠ¨æ€å†…å®¹
- ğŸ“ **TypeScript å¼€å‘**ï¼šç±»å‹å®‰å…¨ï¼Œä»£ç å¯ç»´æŠ¤æ€§å¼º
- âš™ï¸ **é…ç½®é©±åŠ¨**ï¼šé€šè¿‡ JSON é…ç½®æ–‡ä»¶å®šä¹‰çˆ¬å–è§„åˆ™ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 
- ğŸ¯ **å¤šé¡µé¢æ”¯æŒ**ï¼šæ”¯æŒåˆ—è¡¨é¡µå’Œè¯¦æƒ…é¡µçš„æ•°æ®æŠ“å–
- ğŸ”§ **çµæ´»é€‰æ‹©å™¨**ï¼šæ”¯æŒ CSS é€‰æ‹©å™¨å’Œå¤šç§æ•°æ®æå–æ–¹å¼
- ğŸŒ **æ— å¤´æ¨¡å¼**ï¼šæ”¯æŒåå°è¿è¡Œï¼Œæé«˜æ€§èƒ½
- ğŸ“Š **ç»“æ„åŒ–è¾“å‡º**ï¼šå°†æŠ“å–ç»“æœä¿å­˜ä¸º JSON æ ¼å¼

## å®‰è£…ä¾èµ–

```bash
npm install
```

## é¡¹ç›®ç»“æ„

```
JS_web_crawler/
â”œâ”€â”€ src/                    # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ crawler.ts         # çˆ¬è™«æ ¸å¿ƒé€»è¾‘
â”‚   â”œâ”€â”€ main.ts           # ç¨‹åºå…¥å£
â”‚   â””â”€â”€ types.ts          # ç±»å‹å®šä¹‰
â”œâ”€â”€ A1/                    # é…ç½®æ–‡ä»¶ç›®å½• A1
â”‚   â””â”€â”€ C1-config.json    # ç¤ºä¾‹é…ç½®æ–‡ä»¶
â”œâ”€â”€ A2/                    # é…ç½®æ–‡ä»¶ç›®å½• A2
â”‚   â””â”€â”€ B1-config.json    # ç¤ºä¾‹é…ç½®æ–‡ä»¶
â”œâ”€â”€ data/                  # æ•°æ®è¾“å‡ºç›®å½•
â”œâ”€â”€ dist/                  # ç¼–è¯‘åçš„ JavaScript æ–‡ä»¶
â””â”€â”€ README.md             # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

## é…ç½®æ–‡ä»¶è¯´æ˜

é…ç½®æ–‡ä»¶é‡‡ç”¨ JSON æ ¼å¼ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

### åŸºæœ¬é…ç½®

```json
{
  "startUrls": ["https://example.com"], // èµ·å§‹ URL åˆ—è¡¨
  "selectors": {
    "listPage": {
      // åˆ—è¡¨é¡µé…ç½®
      "items": "a.item-link", // åˆ—è¡¨é¡¹é€‰æ‹©å™¨
      "maxPages": 1, // æœ€å¤§æŠ“å–é¡µæ•°
      "itemLink": {
        "selector": ".", // é“¾æ¥é€‰æ‹©å™¨
        "extract": "attribute", // æå–æ–¹å¼
        "attribute": "href" // æå–å±æ€§
      }
    },
    "detailPage": {
      // è¯¦æƒ…é¡µé…ç½®
      "fields": {
        "title": {
          "selector": "h1", // æ ‡é¢˜é€‰æ‹©å™¨
          "extract": "text" // æå–æ–‡æœ¬
        },
        "content": {
          "selector": ".content", // å†…å®¹é€‰æ‹©å™¨
          "extract": "text"
        }
      }
    }
  },
  "behavior": {
    // è¡Œä¸ºé…ç½®
    "headless": true, // æ— å¤´æ¨¡å¼
    "requestDelay": 1500, // è¯·æ±‚å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
    "timeout": 60000, // è¶…æ—¶æ—¶é—´
    "javascript": true, // å¯ç”¨ JavaScript
    "browserType": "chromium" // æµè§ˆå™¨ç±»å‹
  }
}
```

## ä½¿ç”¨æ–¹æ³•

### 1. ç¼–è¯‘é¡¹ç›®

```bash
npm run build
```

### 2. è¿è¡Œçˆ¬è™«

```bash
# è¿è¡Œå•ä¸ªé…ç½®æ–‡ä»¶
node dist/main.js A1/C1-config.json

# æˆ–ä½¿ç”¨ npm è„šæœ¬
npm start A1/C1-config.json
```

### 3. å¼€å‘æ¨¡å¼è¿è¡Œ

```bash
npm run dev A1/C1-config.json
```

## è¾“å‡ºç»“æœ

çˆ¬å–çš„æ•°æ®å°†ä¿å­˜åœ¨ `data/` ç›®å½•ä¸‹ï¼Œæ–‡ä»¶åä¸é…ç½®æ–‡ä»¶åå¯¹åº”ã€‚ä¾‹å¦‚ï¼š

- `A1/C1-config.json` â†’ `data/C1.json`
- `A2/B1-config.json` â†’ `data/B1.json`

## æ³¨æ„äº‹é¡¹

- è¯·éµå®ˆç›®æ ‡ç½‘ç«™çš„ robots.txt å’Œä½¿ç”¨æ¡æ¬¾
- åˆç†è®¾ç½®è¯·æ±‚å»¶è¿Ÿï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆè¿‡å¤§å‹åŠ›
- å»ºè®®åœ¨å¼€å‘æ—¶å…ˆä½¿ç”¨ `headless: false` è¿›è¡Œè°ƒè¯•
- ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ `headless: true` æé«˜æ€§èƒ½

---

## English Usage Guide

### Quick Start

1. **Install dependencies**:

   ```bash
   npm install
   ```

2. **Build the project**:

   ```bash
   npm run build
   ```

3. **Run the crawler**:
   ```bash
   node dist/main.js path/to/config.json
   ```

### Configuration Structure

The crawler uses JSON configuration files to define scraping rules:

- `startUrls`: Array of starting URLs
- `selectors.listPage`: Configuration for list pages (item links extraction)
- `selectors.detailPage`: Configuration for detail pages (data extraction)
- `behavior`: Browser behavior settings (headless mode, delays, timeouts)

### Output

Scraped data is saved as JSON files in the `data/` directory. The filename corresponds to the configuration file name.

### Development

For development and debugging, you can run:

```bash
npm run dev path/to/config.json
```

This project is built with TypeScript and Playwright, providing a robust and flexible web scraping solution.
